{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### DESCRIPCIÓN DE LOS DATOS\n",
    "---\n",
    "\n",
    "En primer lugar vamos a leer un dataset con eventos de cada uno de los usos finales del agua.\n",
    "\n",
    "Este dataset contiene eventos pertenecientes a estos 6 usos finales:\n",
    "\n",
    "* Grifos\n",
    "* Cisternas\n",
    "* Duchas\n",
    "* Fugas\n",
    "* Lavadora\n",
    "* Lavavajillas\n",
    "\n",
    "De cada evento se han extraido 37 variables.\n",
    "\n",
    "El fichero está en format CSV, a continuación se suminastra una función que permite cargarla.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "def ReadEvents(file):\n",
    "    data = np.loadtxt(file,skiprows=1,delimiter=';',usecols=range(0,37))\n",
    "    labels = np.loadtxt(file,skiprows=1,delimiter=';',usecols=(37,),dtype='str')\n",
    "    \n",
    "    (nSamples,nFeatures)=data.shape\n",
    "    randomPermutation = np.random.permutation(nSamples)\n",
    "    data=data[randomPermutation,:]\n",
    "    labels=labels[randomPermutation]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.unique(labels))\n",
    "    labels = le.transform(labels)\n",
    "    return data,labels,le\n",
    "\n",
    "(data,labels,le) = ReadEvents('dataset/dataset_eventos.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### TSNE para entender la complejidad del problema\n",
    "---\n",
    "\n",
    "Vamos a comprobar la dificultad del problema al que nos enfrentamos. Para ello vamos a utilizar el algoritmo de visualización t-SNE.\n",
    "\n",
    "Es un algoritmo que permite reducir datasets con un número elevado de dimensiones a únicamente dos dimensiones, con lo que se puede visualizar cómo de complejo es clasificar un dataset.\n",
    "\n",
    "En la siguiente imágen se puede ver un ejemplo del algoritmo t-SNE con la base de datos MNIST:\n",
    "\n",
    "<img src=\"images/tsne-mnist.png\" width=\"300\">\n",
    "\n",
    "Vamos a ejecutar el algoritmo sobre un conjunto de variables de nuestro dataset para ver la dificultad de nuestro problema.\n",
    "\n",
    "Podrá observar que la principal dificultad radica en el consumo de grifos, que debido a su gran variabilidad puede confundirse con cualquier otro.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "(data,labels,le) = ReadEvents('dataset/dataset_eventos.csv')\n",
    "\n",
    "target_ids=range(1+np.max(labels))\n",
    "class_names = le.inverse_transform(range(1+np.max(labels)))\n",
    "\n",
    "normalizador = Normalizer()\n",
    "data = data[:,[2,9,0,6,4,10,1,7,3]]\n",
    "data=normalizador.fit_transform(data)\n",
    "\n",
    "tsne = TSNE(n_components=2, learning_rate=0.05)\n",
    "\n",
    "X_2d = tsne.fit_transform(data[0:1000,:])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "colors = 'r', 'g', 'b', 'c', 'm', 'k'\n",
    "for i, c, label in zip(target_ids, colors, class_names):\n",
    "    plt.scatter(X_2d[labels[0:1000] == i, 0], X_2d[labels[0:1000] == i, 1], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ALGORITMO 1.MODELO LINEAL, Regresión logística\n",
    "---\n",
    "\n",
    "En primer lugar vamos a probar el clasificador conocido como regresión logística. Es un modelo lineal que se encuentra implementado en la librería sklearn.\n",
    "\n",
    "<img src=\"images/linear.jpg\" width=\"350\" />\n",
    "\n",
    "Su principal hiperparámetro es el asociado a la regularización (parámetro C en la inicialización).\n",
    "\n",
    "La función LogistricRegressionCV se encarga de hacer la validación cruzada por nosotros:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "\n",
    "Siga los siguientes pasos:\n",
    "\n",
    "* Entrene el algoritmo probando los valores C=10^n donde n puede tomar los valores {-4,-3,-2,-1,0,1,2,3,4}\n",
    "* Obtenga el accuracy sobre el conjunto de test.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.4945388349514563\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Vamos a leer los eventos del fichero\n",
    "(data,labels,le) = ReadEvents('dataset/dataset_eventos.csv')\n",
    "\n",
    "##########################################################################################\n",
    "# Divida el conjunto de datos en un conjunto de entrenamiento y test.\n",
    "# Deje el 70% de los datos en el conjunto de entrenamiento y el 30% restante en el de test\n",
    "# Puede hacer uso de la función train_test_split de sklearn\n",
    "##########################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,labels, test_size=0.3)\n",
    "\n",
    "##########################################################################################\n",
    "# Utilice el algoritmo LogisticRegressionCV de sklearn para hacer la validación cruzada\n",
    "# del parámetro C del algoritmo. Puede encontrar aquí información sobre la función \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\n",
    "##########################################################################################\n",
    "\n",
    "cs=[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "\n",
    "modelo = LogisticRegressionCV(cv=5, Cs=cs, multi_class='multinomial').fit(x_train, y_train)\n",
    "\n",
    "###########################################################################################\n",
    "# Utilice el modelo lineal para clasificar el conjunto de test y obtenga la accuracy.\n",
    "###########################################################################################\n",
    "\n",
    "preds = modelo.predict(x_test)\n",
    "accuracy = accuracy_score(y_test,preds) \n",
    "\n",
    "print(\"Accuracy\",accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ALGORITMO 2.MÉTODOS DE NÚCLEO, SVM No lineal\n",
    "---\n",
    "\n",
    "Finalmente vamos a probar un modelo no lineal conocido como máquinas de vectores soporte (SVMs), vamos a utilizar como función de kernel una función de base radial ('rbf'). \n",
    "\n",
    "<img src=\"images/svm.png\" width=\"350\" />\n",
    "\n",
    "También se encuentra en la librería sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "En este caso tenemos dos hiperparámetros a validar. El parámetro de regularización (llamado C) y la anchura de la función de kernel rbf (llamado gamma). \n",
    "\n",
    "La función GridSearchCV nos facilitará la validación cruzada:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "Siga los siguientes pasos:\n",
    "\n",
    "1. Utilice el comando GridSearchCV para obtener los mejores parámetros de C y gamma (pruebe los valores 1, 10 y 100). \n",
    "\n",
    "2. Obtenga las prestaciones sobre el conjunto detest.\n",
    "\n",
    "3. Repita el ejercicio utilizando únicamente las variables [2,9,0,6,4,10,1,7,3]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   1.2s\n",
      "[CV] C=1, gamma=1 ....................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   1.2s\n",
      "[CV] C=1, gamma=10 ...................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   1.2s\n",
      "[CV] C=1, gamma=10 ...................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   1.2s\n",
      "[CV] C=1, gamma=10 ...................................................\n",
      "[CV] .................................... C=1, gamma=10, total=   1.1s\n",
      "[CV] C=1, gamma=10 ...................................................\n",
      "[CV] .................................... C=1, gamma=10, total=   1.1s\n",
      "[CV] C=1, gamma=10 ...................................................\n",
      "[CV] .................................... C=1, gamma=10, total=   1.1s\n",
      "[CV] C=1, gamma=100 ..................................................\n",
      "[CV] ..................................... C=1, gamma=1, total=   1.2s\n",
      "[CV] C=1, gamma=100 ..................................................\n",
      "[CV] ................................... C=1, gamma=100, total=   1.0s\n",
      "[CV] C=1, gamma=100 ..................................................\n",
      "[CV] .................................... C=1, gamma=10, total=   1.1s\n",
      "[CV] C=1, gamma=100 ..................................................\n",
      "[CV] .................................... C=1, gamma=10, total=   1.1s\n",
      "[CV] C=1, gamma=100 ..................................................\n",
      "[CV] ................................... C=1, gamma=100, total=   1.1s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] ................................... C=1, gamma=100, total=   1.0s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] ................................... C=1, gamma=100, total=   1.0s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] ................................... C=1, gamma=100, total=   1.0s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   1.2s\n",
      "[CV] C=10, gamma=1 ...................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   1.2s\n",
      "[CV] C=10, gamma=10 ..................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   1.2s\n",
      "[CV] C=10, gamma=10 ..................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   1.2s\n",
      "[CV] C=10, gamma=10 ..................................................\n",
      "[CV] .................................... C=10, gamma=1, total=   1.2s\n",
      "[CV] C=10, gamma=10 ..................................................\n",
      "[CV] ................................... C=10, gamma=10, total=   1.2s\n",
      "[CV] C=10, gamma=10 ..................................................\n",
      "[CV] ................................... C=10, gamma=10, total=   1.1s\n",
      "[CV] C=10, gamma=100 .................................................\n",
      "[CV] ................................... C=10, gamma=10, total=   1.2s\n",
      "[CV] C=10, gamma=100 .................................................\n",
      "[CV] ................................... C=10, gamma=10, total=   1.2s\n",
      "[CV] C=10, gamma=100 .................................................\n",
      "[CV] ................................... C=10, gamma=10, total=   1.2s\n",
      "[CV] C=10, gamma=100 .................................................\n",
      "[CV] .................................. C=10, gamma=100, total=   1.2s\n",
      "[CV] C=10, gamma=100 .................................................\n",
      "[CV] .................................. C=10, gamma=100, total=   1.2s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] .................................. C=10, gamma=100, total=   1.2s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] .................................. C=10, gamma=100, total=   1.2s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] .................................. C=10, gamma=100, total=   1.3s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   1.3s\n",
      "[CV] C=100, gamma=1 ..................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   1.2s\n",
      "[CV] C=100, gamma=10 .................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   1.2s\n",
      "[CV] C=100, gamma=10 .................................................\n",
      "[CV] ................................... C=100, gamma=1, total=   1.3s\n",
      "[CV] C=100, gamma=10 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   16.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................... C=100, gamma=1, total=   1.3s\n",
      "[CV] C=100, gamma=10 .................................................\n",
      "[CV] .................................. C=100, gamma=10, total=   1.8s\n",
      "[CV] C=100, gamma=10 .................................................\n",
      "[CV] .................................. C=100, gamma=10, total=   1.7s\n",
      "[CV] C=100, gamma=100 ................................................\n",
      "[CV] .................................. C=100, gamma=10, total=   1.6s\n",
      "[CV] C=100, gamma=100 ................................................\n",
      "[CV] .................................. C=100, gamma=10, total=   1.6s\n",
      "[CV] C=100, gamma=100 ................................................\n",
      "[CV] .................................. C=100, gamma=10, total=   1.7s\n",
      "[CV] C=100, gamma=100 ................................................\n",
      "[CV] ................................. C=100, gamma=100, total=   1.7s\n",
      "[CV] C=100, gamma=100 ................................................\n",
      "[CV] ................................. C=100, gamma=100, total=   1.8s\n",
      "[CV] ................................. C=100, gamma=100, total=   1.6s\n",
      "[CV] ................................. C=100, gamma=100, total=   1.8s\n",
      "[CV] ................................. C=100, gamma=100, total=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:   23.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.6462378640776699\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "(data,labels,le) = ReadEvents('dataset/dataset_eventos.csv')\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,labels, test_size=0.3)\n",
    "\n",
    "\n",
    "\n",
    "x_train=x_train[:,[2,9,0,6,4,10,1,7,3]]\n",
    "x_test=x_test[:,[2,9,0,6,4,10,1,7,3]]\n",
    "\n",
    "######################################################################################\n",
    "# Nomalice el conjunto de datos para que las vairables tengan media 0 y varianza 1\n",
    "# Puede hacer uso de la función Normalizer de sklearn.\n",
    "#######################################################################################\n",
    "normalizador = Normalizer()\n",
    "x_train=normalizador.fit_transform(x_train)\n",
    "x_test=normalizador.transform(x_test)\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Hago uso de la función GridSearchCV para entrenar una SVM\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# Explore los valores de C=1, 10, 100 y gamma=1,10,100 \n",
    "######################################################################################\n",
    "\n",
    "Cs = [1, 10, 100]\n",
    "gammas = [1,10,100]\n",
    "param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "svm_classifier = GridSearchCV(SVC(kernel='rbf',cache_size=4000), param_grid, cv=StratifiedKFold(n_splits=5),n_jobs=4,verbose=2)\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "\n",
    "###########################################################################################\n",
    "# Utilice la SVM para clasificar el conjunto de test y obtenga la accuracy.\n",
    "###########################################################################################\n",
    "\n",
    "\n",
    "predictions=svm_classifier.predict(x_test)\n",
    "Accuracy = accuracy_score(predictions,y_test)\n",
    "\n",
    "print(\"Final Accuracy:\", Accuracy)\n",
    "\n",
    "###############################################################################################\n",
    "# No se observa mejora respecto al modelo lineal. El motivo es que a la SVM le afectan muy negativamente\n",
    "# varaibles que no aportan información.\n",
    "# Repita el ejercicio seleccionando las variables (columnas) [2,9,0,6,4,10,1,7,3]\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ALGORITMO 3. REDES NEURONALES, Perceptrón Multicapa\n",
    "---\n",
    "\n",
    "\n",
    "El tercer algoritmo que vamos a probar es una red neuronal. Para ello vamos a hacer uso de dos librerías de Python:\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/?hl=es)\n",
    "\n",
    "[Keras](https://keras.io/)\n",
    "\n",
    "\n",
    "#### DEFINICIÓN DE LA RED NEURONAL\n",
    "\n",
    "\n",
    "Imaginemos que necesitamos implementar la red neuronal de la siguiente figura (capa de entrada 37 neuronas, primera capa oculta 4 neuronas, segunda capa oculta 3 neuronas, capa de salida 6 neuronas (una por cada uso de agua):\n",
    "\n",
    "<img src=\"images/MLP.png\" alt=\"mnist\" width=\"500\"/>\n",
    "\n",
    "Con la librería keras, es muy sencillo, vamos a emplear un modelo secuencial donde las capas de neuronas se van añadiendo una a una. Para un perceptrón multicapa tenemos que ir añadiendo capas de tipo Dense (conectan todas las neuronas de la capa anterior con la capa siguiente, se indica el númeor de neuronas de la siguiente capa y la función de activación que aplican):\n",
    "\n",
    "```Python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Instanciamos la red neuronal\n",
    "model = Sequential()\n",
    "\n",
    "# Añadimos la primera capa oculta, numero de neuronas y función de activación.\n",
    "# NOTA: En la primera capa tenemos que indicar el tamaño de los datos de entrada.\n",
    "model.add(Dense(100, input_shape=(37,), activation='tanh'))\n",
    "\n",
    "# Añadimos la primera capa oculta, numero de neuronas y función de activación.\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "```\n",
    "\n",
    "**Recuerde que para la capa de salida:**\n",
    "\n",
    "* Para problemas de regresión (la salida es una variable continua) no debe aplicar ninguna función de activación. \n",
    "* Para problemas de clasificación, donde hay varias categorías de clasificación y un dato solo puede pertenecer a una de las clases, entonces debe haber tantas neuronas como clases y la función de activación de la capa de salida debe ser softmax para indicar que queremos probabilidades y que la suma de ellas debe ser igual a uno. \n",
    "* Para problemas de clasificación multiclase, donde hay varias categorías y un dato puede pertenecer a la vez a varias de ellas, debe aplicarse una función de activación sigmoide. \n",
    "\n",
    "#### DEFINICIÓN DE LA FUNCIÓN DE COSTE Y EL ALGORITMO DE OPTIMIZACIÓN\n",
    "\n",
    "El siguiente paso es definir la función de coste, el algoritmo de optimización y la información que deseamos que se muestre por pantalla durante el entrenamiento. Todo eso se hace utilizando una única línea de código.\n",
    "\n",
    "**Para las funciones de coste, recuerde de la teoría que hemos visto que:**\n",
    "\n",
    "* Para clasificación multiclase donde los datos pueden pertenecer a una única clase se utiliza (este es nuestro caso): 'categorical_crossentropy'\n",
    "* Para clasificación multiclase donde los datos pueden pertenecer a varias categorías se utiliza: 'binary_crossentropy'\n",
    "* Para regresión se utilizan funciones como: 'mean_squared_error', 'mean_absolute_error'\n",
    "\n",
    "**Respecto al algoritmos de optimización:**\n",
    "\n",
    "* Hay muchas alternativas, cada una tiene sus ventajas e inconvenientes y no es posible conocer de antemano el mejor algoritmo de optimización. Aquí tiene algunos ejemplos visuales para mostrar cómo algunos funcionan mejor o peor dependiendo de la situación a la hora de encontrar el mínimode la función de coste:\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "  <td><img src=\"images/1.gif\" width=\"200\"/></td><td><img src=\"images/4.gif\" width=\"200\" /></td>\n",
    "  </tr>     \n",
    "       <tr>\n",
    "  <td><img src=\"images/2.gif\" width=\"200\"/></td><td><img src=\"images/3.gif\" width=\"200\" /></td>\n",
    "  </tr>     \n",
    "\n",
    "</table>\n",
    "\n",
    "**Respecto a la información a mostrar por pantalla:**\n",
    "Lo normal en problemas de clasificación como el que vamos a tratar es mostrar el ratio de (aciertos)/(número total de datos). Aquí puede encontrar información sobre más métricas, puedeseleccionar más de una:\n",
    "[https://keras.io/metrics/]( https://keras.io/metrics/)\n",
    "\n",
    "**Aquí tiene un ejemplo de cómo se define:**\n",
    "\n",
    "```Python\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=nadam,\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters: lrate= 0.0001\n",
      "Train on 5381 samples, validate on 2307 samples\n",
      "Epoch 1/40\n",
      "5381/5381 [==============================] - 1s 205us/step - loss: 1.5372 - acc: 0.4746 - val_loss: 1.4154 - val_acc: 0.4764\n",
      "Epoch 2/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 1.3888 - acc: 0.4789 - val_loss: 1.3662 - val_acc: 0.4764\n",
      "Epoch 3/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 1.3297 - acc: 0.4804 - val_loss: 1.3018 - val_acc: 0.4803\n",
      "Epoch 4/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 1.2670 - acc: 0.4895 - val_loss: 1.2445 - val_acc: 0.5054\n",
      "Epoch 5/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 1.2132 - acc: 0.5088 - val_loss: 1.1964 - val_acc: 0.5085\n",
      "Epoch 6/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 1.1704 - acc: 0.5174 - val_loss: 1.1629 - val_acc: 0.5106\n",
      "Epoch 7/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 1.1370 - acc: 0.5241 - val_loss: 1.1317 - val_acc: 0.5180\n",
      "Epoch 8/40\n",
      "5381/5381 [==============================] - 1s 126us/step - loss: 1.1105 - acc: 0.5259 - val_loss: 1.1088 - val_acc: 0.5219\n",
      "Epoch 9/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 1.0882 - acc: 0.5315 - val_loss: 1.0923 - val_acc: 0.5236\n",
      "Epoch 10/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 1.0715 - acc: 0.5408 - val_loss: 1.0749 - val_acc: 0.5306\n",
      "Epoch 11/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 1.0558 - acc: 0.5447 - val_loss: 1.0610 - val_acc: 0.5397\n",
      "Epoch 12/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 1.0416 - acc: 0.5536 - val_loss: 1.0515 - val_acc: 0.5548\n",
      "Epoch 13/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 1.0303 - acc: 0.5584 - val_loss: 1.0363 - val_acc: 0.5674\n",
      "Epoch 14/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 1.0187 - acc: 0.5685 - val_loss: 1.0254 - val_acc: 0.5687\n",
      "Epoch 15/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 1.0097 - acc: 0.5622 - val_loss: 1.0177 - val_acc: 0.5674\n",
      "Epoch 16/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 1.0009 - acc: 0.5705 - val_loss: 1.0086 - val_acc: 0.5739\n",
      "Epoch 17/40\n",
      "5381/5381 [==============================] - 1s 115us/step - loss: 0.9930 - acc: 0.5742 - val_loss: 0.9987 - val_acc: 0.5769\n",
      "Epoch 18/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9837 - acc: 0.5776 - val_loss: 0.9948 - val_acc: 0.5813\n",
      "Epoch 19/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.9772 - acc: 0.5794 - val_loss: 0.9845 - val_acc: 0.5821\n",
      "Epoch 20/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.9698 - acc: 0.5835 - val_loss: 0.9804 - val_acc: 0.5722\n",
      "Epoch 21/40\n",
      "5381/5381 [==============================] - 1s 108us/step - loss: 0.9632 - acc: 0.5837 - val_loss: 0.9716 - val_acc: 0.5943\n",
      "Epoch 22/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.9559 - acc: 0.5863 - val_loss: 0.9618 - val_acc: 0.5956\n",
      "Epoch 23/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9498 - acc: 0.5889 - val_loss: 0.9569 - val_acc: 0.6047\n",
      "Epoch 24/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9428 - acc: 0.5956 - val_loss: 0.9491 - val_acc: 0.5977\n",
      "Epoch 25/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.9372 - acc: 0.5936 - val_loss: 0.9449 - val_acc: 0.6103\n",
      "Epoch 26/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.9318 - acc: 0.6017 - val_loss: 0.9375 - val_acc: 0.6047\n",
      "Epoch 27/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.9261 - acc: 0.6025 - val_loss: 0.9320 - val_acc: 0.6177\n",
      "Epoch 28/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9216 - acc: 0.6112 - val_loss: 0.9267 - val_acc: 0.6177\n",
      "Epoch 29/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9165 - acc: 0.6142 - val_loss: 0.9219 - val_acc: 0.6251\n",
      "Epoch 30/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9112 - acc: 0.6149 - val_loss: 0.9214 - val_acc: 0.6307\n",
      "Epoch 31/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9067 - acc: 0.6222 - val_loss: 0.9137 - val_acc: 0.6233\n",
      "Epoch 32/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9022 - acc: 0.6237 - val_loss: 0.9070 - val_acc: 0.6415\n",
      "Epoch 33/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.8984 - acc: 0.6291 - val_loss: 0.9036 - val_acc: 0.6324\n",
      "Epoch 34/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.8935 - acc: 0.6309 - val_loss: 0.8996 - val_acc: 0.6389\n",
      "Epoch 35/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.8900 - acc: 0.6319 - val_loss: 0.8958 - val_acc: 0.6450\n",
      "Epoch 36/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.8854 - acc: 0.6359 - val_loss: 0.8928 - val_acc: 0.6402\n",
      "Epoch 37/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.8827 - acc: 0.6324 - val_loss: 0.8878 - val_acc: 0.6459\n",
      "Epoch 38/40\n",
      "5381/5381 [==============================] - 1s 109us/step - loss: 0.8793 - acc: 0.6352 - val_loss: 0.8829 - val_acc: 0.6537\n",
      "Epoch 39/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.8754 - acc: 0.6382 - val_loss: 0.8816 - val_acc: 0.6502\n",
      "Epoch 40/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.8715 - acc: 0.6391 - val_loss: 0.8809 - val_acc: 0.6463\n",
      "Right Now Best Accuracy CV: 0.64629388816645 LRate: 0.0001\n",
      "Training parameters: lrate= 0.001\n",
      "Train on 5381 samples, validate on 2307 samples\n",
      "Epoch 1/40\n",
      "5381/5381 [==============================] - 1s 137us/step - loss: 1.2863 - acc: 0.4949 - val_loss: 1.1128 - val_acc: 0.5128\n",
      "Epoch 2/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 1.0450 - acc: 0.5462 - val_loss: 1.0135 - val_acc: 0.5652\n",
      "Epoch 3/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9758 - acc: 0.5731 - val_loss: 0.9901 - val_acc: 0.5401\n",
      "Epoch 4/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.9338 - acc: 0.6006 - val_loss: 0.9617 - val_acc: 0.6021\n",
      "Epoch 5/40\n",
      "5381/5381 [==============================] - 1s 110us/step - loss: 0.9037 - acc: 0.6226 - val_loss: 0.8887 - val_acc: 0.6281\n",
      "Epoch 6/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.8731 - acc: 0.6369 - val_loss: 0.9661 - val_acc: 0.6008\n",
      "Epoch 7/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.8570 - acc: 0.6391 - val_loss: 0.8703 - val_acc: 0.6415\n",
      "Epoch 8/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.8415 - acc: 0.6482 - val_loss: 0.8190 - val_acc: 0.6654\n",
      "Epoch 9/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.8275 - acc: 0.6553 - val_loss: 0.8436 - val_acc: 0.6593\n",
      "Epoch 10/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.8174 - acc: 0.6547 - val_loss: 0.8150 - val_acc: 0.6749\n",
      "Epoch 11/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.8055 - acc: 0.6668 - val_loss: 0.8268 - val_acc: 0.6641\n",
      "Epoch 12/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7942 - acc: 0.6735 - val_loss: 0.8320 - val_acc: 0.6389\n",
      "Epoch 13/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7853 - acc: 0.6744 - val_loss: 0.8039 - val_acc: 0.6632\n",
      "Epoch 14/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7727 - acc: 0.6768 - val_loss: 0.8768 - val_acc: 0.6459\n",
      "Epoch 15/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7682 - acc: 0.6774 - val_loss: 0.7614 - val_acc: 0.6953\n",
      "Epoch 16/40\n",
      "5381/5381 [==============================] - 1s 114us/step - loss: 0.7585 - acc: 0.6835 - val_loss: 0.7860 - val_acc: 0.6853\n",
      "Epoch 17/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7489 - acc: 0.6893 - val_loss: 0.7709 - val_acc: 0.6918\n",
      "Epoch 18/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.7404 - acc: 0.6949 - val_loss: 0.7561 - val_acc: 0.7070\n",
      "Epoch 19/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7290 - acc: 0.6997 - val_loss: 0.7585 - val_acc: 0.6957\n",
      "Epoch 20/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7217 - acc: 0.7053 - val_loss: 0.7375 - val_acc: 0.6909\n",
      "Epoch 21/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7138 - acc: 0.7056 - val_loss: 0.7707 - val_acc: 0.6875\n",
      "Epoch 22/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7049 - acc: 0.7142 - val_loss: 0.7450 - val_acc: 0.7096\n",
      "Epoch 23/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6950 - acc: 0.7229 - val_loss: 0.7420 - val_acc: 0.7057\n",
      "Epoch 24/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6854 - acc: 0.7246 - val_loss: 0.7769 - val_acc: 0.6636\n",
      "Epoch 25/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6791 - acc: 0.7268 - val_loss: 0.7135 - val_acc: 0.7152\n",
      "Epoch 26/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6723 - acc: 0.7344 - val_loss: 0.7012 - val_acc: 0.7204\n",
      "Epoch 27/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6661 - acc: 0.7339 - val_loss: 0.7041 - val_acc: 0.7152\n",
      "Epoch 28/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6588 - acc: 0.7343 - val_loss: 0.6915 - val_acc: 0.7208\n",
      "Epoch 29/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6531 - acc: 0.7421 - val_loss: 0.6898 - val_acc: 0.7295\n",
      "Epoch 30/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6449 - acc: 0.7382 - val_loss: 0.6865 - val_acc: 0.7226\n",
      "Epoch 31/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6436 - acc: 0.7445 - val_loss: 0.6737 - val_acc: 0.7308\n",
      "Epoch 32/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6346 - acc: 0.7463 - val_loss: 0.6929 - val_acc: 0.7208\n",
      "Epoch 33/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6293 - acc: 0.7474 - val_loss: 0.7278 - val_acc: 0.7057\n",
      "Epoch 34/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6291 - acc: 0.7458 - val_loss: 0.6713 - val_acc: 0.7412\n",
      "Epoch 35/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6202 - acc: 0.7495 - val_loss: 0.6856 - val_acc: 0.7282\n",
      "Epoch 36/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6212 - acc: 0.7474 - val_loss: 0.6638 - val_acc: 0.7365\n",
      "Epoch 37/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6158 - acc: 0.7486 - val_loss: 0.6481 - val_acc: 0.7460\n",
      "Epoch 38/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6120 - acc: 0.7586 - val_loss: 0.6644 - val_acc: 0.7404\n",
      "Epoch 39/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6064 - acc: 0.7593 - val_loss: 0.6684 - val_acc: 0.7356\n",
      "Epoch 40/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6030 - acc: 0.7592 - val_loss: 0.6741 - val_acc: 0.7269\n",
      "Right Now Best Accuracy CV: 0.7269180754226268 LRate: 0.001\n",
      "Training parameters: lrate= 0.01\n",
      "Train on 5381 samples, validate on 2307 samples\n",
      "Epoch 1/40\n",
      "5381/5381 [==============================] - 1s 142us/step - loss: 1.1720 - acc: 0.5138 - val_loss: 1.0266 - val_acc: 0.5444\n",
      "Epoch 2/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.9979 - acc: 0.5636 - val_loss: 1.2788 - val_acc: 0.4421\n",
      "Epoch 3/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.9292 - acc: 0.6086 - val_loss: 0.8935 - val_acc: 0.5986\n",
      "Epoch 4/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.8673 - acc: 0.6294 - val_loss: 0.8270 - val_acc: 0.6688\n",
      "Epoch 5/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.8232 - acc: 0.6631 - val_loss: 0.8200 - val_acc: 0.6329\n",
      "Epoch 6/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.7848 - acc: 0.6805 - val_loss: 0.8410 - val_acc: 0.6398\n",
      "Epoch 7/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.7554 - acc: 0.6867 - val_loss: 0.7456 - val_acc: 0.7026\n",
      "Epoch 8/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.7413 - acc: 0.6941 - val_loss: 1.1997 - val_acc: 0.5505\n",
      "Epoch 9/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.7218 - acc: 0.7056 - val_loss: 0.8211 - val_acc: 0.6480\n",
      "Epoch 10/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.7021 - acc: 0.7069 - val_loss: 1.0639 - val_acc: 0.5713\n",
      "Epoch 11/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6892 - acc: 0.7199 - val_loss: 0.8276 - val_acc: 0.6719\n",
      "Epoch 12/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6830 - acc: 0.7203 - val_loss: 0.8034 - val_acc: 0.6580\n",
      "Epoch 13/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6753 - acc: 0.7240 - val_loss: 0.7447 - val_acc: 0.6823\n",
      "Epoch 14/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6548 - acc: 0.7339 - val_loss: 0.6758 - val_acc: 0.7204\n",
      "Epoch 15/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6555 - acc: 0.7277 - val_loss: 0.7020 - val_acc: 0.7200\n",
      "Epoch 16/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6505 - acc: 0.7382 - val_loss: 0.7138 - val_acc: 0.7191\n",
      "Epoch 17/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6406 - acc: 0.7445 - val_loss: 0.7620 - val_acc: 0.6961\n",
      "Epoch 18/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6436 - acc: 0.7393 - val_loss: 0.7426 - val_acc: 0.7187\n",
      "Epoch 19/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6329 - acc: 0.7467 - val_loss: 0.6688 - val_acc: 0.7313\n",
      "Epoch 20/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6291 - acc: 0.7454 - val_loss: 0.7304 - val_acc: 0.7104\n",
      "Epoch 21/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6202 - acc: 0.7467 - val_loss: 0.6723 - val_acc: 0.7395\n",
      "Epoch 22/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6214 - acc: 0.7478 - val_loss: 0.7019 - val_acc: 0.7221\n",
      "Epoch 23/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6165 - acc: 0.7450 - val_loss: 0.6709 - val_acc: 0.7412\n",
      "Epoch 24/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6077 - acc: 0.7534 - val_loss: 0.6729 - val_acc: 0.7369\n",
      "Epoch 25/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.6048 - acc: 0.7506 - val_loss: 0.7443 - val_acc: 0.7126\n",
      "Epoch 26/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.6035 - acc: 0.7566 - val_loss: 0.6704 - val_acc: 0.7399\n",
      "Epoch 27/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.6011 - acc: 0.7551 - val_loss: 0.7059 - val_acc: 0.7235\n",
      "Epoch 28/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.5955 - acc: 0.7571 - val_loss: 0.8945 - val_acc: 0.6441\n",
      "Epoch 29/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.5931 - acc: 0.7575 - val_loss: 0.6871 - val_acc: 0.7365\n",
      "Epoch 30/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5941 - acc: 0.7586 - val_loss: 0.6736 - val_acc: 0.7330\n",
      "Epoch 31/40\n",
      "5381/5381 [==============================] - 1s 114us/step - loss: 0.5859 - acc: 0.7610 - val_loss: 0.6754 - val_acc: 0.7378\n",
      "Epoch 32/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5852 - acc: 0.7642 - val_loss: 0.6781 - val_acc: 0.7447\n",
      "Epoch 33/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5796 - acc: 0.7642 - val_loss: 0.6679 - val_acc: 0.7473\n",
      "Epoch 34/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5773 - acc: 0.7636 - val_loss: 0.6781 - val_acc: 0.7434\n",
      "Epoch 35/40\n",
      "5381/5381 [==============================] - 1s 113us/step - loss: 0.5745 - acc: 0.7642 - val_loss: 0.7392 - val_acc: 0.7178\n",
      "Epoch 36/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5719 - acc: 0.7664 - val_loss: 0.6731 - val_acc: 0.7408\n",
      "Epoch 37/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.5633 - acc: 0.7714 - val_loss: 0.7080 - val_acc: 0.7282\n",
      "Epoch 38/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5672 - acc: 0.7649 - val_loss: 0.7318 - val_acc: 0.7161\n",
      "Epoch 39/40\n",
      "5381/5381 [==============================] - 1s 111us/step - loss: 0.5755 - acc: 0.7666 - val_loss: 0.7272 - val_acc: 0.7269\n",
      "Epoch 40/40\n",
      "5381/5381 [==============================] - 1s 112us/step - loss: 0.5691 - acc: 0.7690 - val_loss: 0.7208 - val_acc: 0.7169\n",
      "Right Now Best Accuracy CV: 0.7269180754226268 LRate: 0.001\n",
      "Epoch 1/40\n",
      "7688/7688 [==============================] - 1s 127us/step - loss: 1.2214 - acc: 0.5052\n",
      "Epoch 2/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 1.0004 - acc: 0.5662\n",
      "Epoch 3/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.9321 - acc: 0.6047\n",
      "Epoch 4/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.8897 - acc: 0.6359\n",
      "Epoch 5/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.8558 - acc: 0.6493\n",
      "Epoch 6/40\n",
      "7688/7688 [==============================] - 1s 101us/step - loss: 0.8312 - acc: 0.6582\n",
      "Epoch 7/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.8103 - acc: 0.6671\n",
      "Epoch 8/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7924 - acc: 0.6752\n",
      "Epoch 9/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7806 - acc: 0.6804\n",
      "Epoch 10/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.7660 - acc: 0.6887\n",
      "Epoch 11/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7495 - acc: 0.6928\n",
      "Epoch 12/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7378 - acc: 0.6980\n",
      "Epoch 13/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7214 - acc: 0.7075\n",
      "Epoch 14/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7100 - acc: 0.7148\n",
      "Epoch 15/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.7017 - acc: 0.7153\n",
      "Epoch 16/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.6879 - acc: 0.7271\n",
      "Epoch 17/40\n",
      "7688/7688 [==============================] - 1s 104us/step - loss: 0.6808 - acc: 0.7294\n",
      "Epoch 18/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.6663 - acc: 0.7366\n",
      "Epoch 19/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6619 - acc: 0.7339\n",
      "Epoch 20/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6543 - acc: 0.7343\n",
      "Epoch 21/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6483 - acc: 0.7440\n",
      "Epoch 22/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6449 - acc: 0.7399\n",
      "Epoch 23/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6386 - acc: 0.7460\n",
      "Epoch 24/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.6322 - acc: 0.7507\n",
      "Epoch 25/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6244 - acc: 0.7529\n",
      "Epoch 26/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6212 - acc: 0.7512\n",
      "Epoch 27/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.6170 - acc: 0.7591\n",
      "Epoch 28/40\n",
      "7688/7688 [==============================] - 1s 104us/step - loss: 0.6125 - acc: 0.7542\n",
      "Epoch 29/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6099 - acc: 0.7601\n",
      "Epoch 30/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6068 - acc: 0.7562\n",
      "Epoch 31/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.6014 - acc: 0.7598\n",
      "Epoch 32/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.5991 - acc: 0.7579\n",
      "Epoch 33/40\n",
      "7688/7688 [==============================] - 1s 104us/step - loss: 0.5939 - acc: 0.7613\n",
      "Epoch 34/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.5918 - acc: 0.7616\n",
      "Epoch 35/40\n",
      "7688/7688 [==============================] - 1s 103us/step - loss: 0.5837 - acc: 0.7693\n",
      "Epoch 36/40\n",
      "7688/7688 [==============================] - 1s 104us/step - loss: 0.5839 - acc: 0.7682\n",
      "Epoch 37/40\n",
      "7688/7688 [==============================] - 1s 105us/step - loss: 0.5810 - acc: 0.7683\n",
      "Epoch 38/40\n",
      "7688/7688 [==============================] - 1s 102us/step - loss: 0.5788 - acc: 0.7689\n",
      "Epoch 39/40\n",
      "7688/7688 [==============================] - 1s 106us/step - loss: 0.5735 - acc: 0.7721\n",
      "Epoch 40/40\n",
      "7688/7688 [==============================] - 1s 110us/step - loss: 0.5741 - acc: 0.7682\n",
      "Accuracy: 0.7463592233009708\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "###################################################################################\n",
    "# Cargamos el dataset y dividimos en conjunto de entrenamiento y test\n",
    "###################################################################################\n",
    "(data,labels,le) = ReadEvents('dataset/dataset_eventos.csv')\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,labels, test_size=0.3)\n",
    "\n",
    "###################################################################################\n",
    "# Normalizamos con un min max scaler\n",
    "###################################################################################\n",
    "normalizer = MinMaxScaler(feature_range=(0, 0.1))\n",
    "x_train=normalizer.fit_transform(x_train)\n",
    "x_test=normalizer.transform(x_test)\n",
    "\n",
    "###################################################################################\n",
    "# Vamos a validar la tasa de aprendizaje, creamos los conjuntos de validación y \n",
    "# el rango de valores del learning rate que vamos a probar\n",
    "###################################################################################\n",
    "Xcvtr, Xcvtst, Ycvtr, Ycvtst = train_test_split(x_train,y_train, test_size=0.3)\n",
    "lrates = [0.0001, 0.001,0.01]\n",
    "   \n",
    "\n",
    "BestAccuracy=-1.0\n",
    "BestC = -1\n",
    "BestG = -1\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# Ponemos las etiquetas en el formato correcto\n",
    "###################################################################################\n",
    "\n",
    "Ycvtr = np.eye(6)[np.int_(Ycvtr)]\n",
    "Ycvtst = np.eye(6)[np.int_(Ycvtst)]\n",
    "y_test = np.eye(6)[np.int_(y_test)]\n",
    "y_train = np.eye(6)[np.int_(y_train)]\n",
    "\n",
    "###################################################################################\n",
    "# Iteramos sobre los valores de la tasa de aprendizaje\n",
    "###################################################################################\n",
    "\n",
    "for l in lrates:\n",
    "    print(\"Training parameters: lrate=\",l)\n",
    "    \n",
    "    ###################################################################################\n",
    "    # Define aquí con keras la red neuronal a utilizar\n",
    "    # Puedes jugar con el número de capas y con las neuronas en cada capa para tratar\n",
    "    # de alcanzar la mayor prestación posible\n",
    "    ###################################################################################\n",
    "    \n",
    "    n_epoch = 40\n",
    "    batch_size = 16\n",
    "    input_dim = 37\n",
    "    hidden = 300\n",
    "    output = 6  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden,input_dim=input_dim,init='lecun_uniform',activation='relu'))\n",
    "    model.add(Dense(hidden,init='lecun_uniform',activation='relu'))\n",
    "    model.add(Dense(output_dim=6,init='lecun_uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "    decay = l/n_epoch\n",
    "\n",
    "    nadam = Nadam(lr=l)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "    model.fit(Xcvtr, Ycvtr, nb_epoch=n_epoch, shuffle=True, batch_size=batch_size,validation_data=(Xcvtst, Ycvtst))\n",
    "            \n",
    "    predictions = model.predict(Xcvtst)\n",
    "    predictions =np.argmax(predictions,axis=1)\n",
    "    labelsTest = np.argmax(Ycvtst,axis= 1)\n",
    "    Accuracy = accuracy_score(predictions,labelsTest)\n",
    "            \n",
    "            \n",
    "    if(Accuracy> BestAccuracy):\n",
    "        BestAccuracy = Accuracy\n",
    "        BestLRate = l\n",
    "                \n",
    "\n",
    "    print(\"Right Now Best Accuracy CV:\",BestAccuracy, \"LRate:\", BestLRate)\n",
    "\n",
    "###################################################################################\n",
    "# VUELVA A DEFINIR AQUÍ LA RED NEURONAL\n",
    "###################################################################################    \n",
    "model = Sequential()\n",
    "model.add(Dense(hidden,input_dim=input_dim,init='lecun_uniform',activation='relu'))\n",
    "model.add(Dense(hidden,init='lecun_uniform',activation='relu'))\n",
    "model.add(Dense(output_dim=6,init='lecun_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "nadam = Nadam(lr=BestLRate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=nadam, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, nb_epoch=n_epoch, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# EVALUAMOS AHORA SOBRE EL CONJUNTO DE TEST\n",
    "###################################################################################  \n",
    "predictions = model.predict(x_test)\n",
    "predictions =np.argmax(predictions,axis=1)\n",
    "labelsTest = np.argmax(y_test,axis= 1)\n",
    "Accuracy = accuracy_score(predictions,labelsTest)\n",
    "print(\"Accuracy:\",Accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ALGORITMO 4. ÁRBOLES, XGBOOST\n",
    "---\n",
    "\n",
    "Tradicionalmente Random Forest ha sido muy utilizado para trabajar con datos que combinan variables continuas con categóricas. Recientemente, otro algoritmo ha pasado a dominar este tipo de problemas: Gradient Boosted Trees.\n",
    "\n",
    "Al igual que RF, GBT clasifica ejemplos mediante el uso de un conjunto de árboles de decisión. En el caso de este segundo, los árboles se construyen secuencialmente, añadiendo en cada iteración el árbol que mejor compense por los errores de los árboles ya existentes. Se le llama método de gradiente porque el modelo evoluciona en dirección al menor error, árbol a árbol.\n",
    "\n",
    "<img src=\"images/tree.png\" width=\"350\" />\n",
    "\n",
    "La librería que vamos a utilizar es XGBoost, una implementación de GBT compatible con Python y R. Su éxito en competiciones se debe no solo a su precisión, sino también a la velocidad de iteración que permite. Aún trabajando con un solo core, funciona dos veces más rápido que la biblioteca gbm de R y cuatro veces más rápido que la implementación de scikit-learn en Python. Esto se consigue mediante una serie de optimizaciones en la implementación, que pueden acelerarse aún más gracias a su compatibilidad con sistemas distribuidos, como por ejemplo MPI y Yarn.\n",
    "\n",
    "Sus principales parámetros a validar son la profundidad de los árboles (parámetro max_depth en la inicialización) y el número de árboles (n_estimators). Para entrenar un modelo se utiliza la siguiente secuencia de comandos:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbclassifier=XGBClassifier(max_depth=..., n_estimators=..., learning_rate=0.05,subsample=0.9,verbose=2)\n",
    "xgbclassifier.fit(Xcvtr,Ycvtr)\n",
    "\n",
    "```\n",
    "Para hacer predicciones se utiliza el comando predict al igual que en algoritmos anteriores.\n",
    "\n",
    "Siga los siguientes pasos:\n",
    "\n",
    "1. Valide los parámetros max_depth probando los valores 3, 7, 11 y el valor n_estimators probando los valores 10, 25 y 100. \n",
    "\n",
    "2. Una vez conocido el mejor valor de los hiperparámetros, entrene el algoritmo utilizando la totalidad del conjunto de entrenamiento.\n",
    "\n",
    "3. Obtenga el accuracy sobre el conjunto de test.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters: depth= 3 estimators= 10\n",
      "Right Now Best Accuracy CV: 0.7200832466181062 Depth: 3\n",
      "Training parameters: depth= 3 estimators= 100\n",
      "Right Now Best Accuracy CV: 0.7793964620187305 Depth: 3\n",
      "Training parameters: depth= 3 estimators= 200\n",
      "Right Now Best Accuracy CV: 0.8111342351716961 Depth: 3\n",
      "Training parameters: depth= 7 estimators= 10\n",
      "Right Now Best Accuracy CV: 0.8111342351716961 Depth: 3\n",
      "Training parameters: depth= 7 estimators= 100\n",
      "Right Now Best Accuracy CV: 0.8459937565036421 Depth: 7\n",
      "Training parameters: depth= 7 estimators= 200\n",
      "Right Now Best Accuracy CV: 0.8590010405827263 Depth: 7\n",
      "Training parameters: depth= 11 estimators= 10\n",
      "Right Now Best Accuracy CV: 0.8590010405827263 Depth: 7\n",
      "Training parameters: depth= 11 estimators= 100\n",
      "Right Now Best Accuracy CV: 0.8621227887617066 Depth: 11\n",
      "Training parameters: depth= 11 estimators= 200\n",
      "Right Now Best Accuracy CV: 0.8621227887617066 Depth: 11\n",
      "Training parameters: depth= 15 estimators= 10\n",
      "Right Now Best Accuracy CV: 0.8621227887617066 Depth: 11\n",
      "Training parameters: depth= 15 estimators= 100\n",
      "Right Now Best Accuracy CV: 0.8636836628511967 Depth: 15\n",
      "Training parameters: depth= 15 estimators= 200\n",
      "Right Now Best Accuracy CV: 0.8636836628511967 Depth: 15\n",
      "Final Accuracy: 0.8831917475728155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "(data,labels,le) = ReadEvents('dataset/dataset_eventos.csv')\n",
    "\n",
    "############################################################################\n",
    "# Dividimos el conjunto de datos en un conjunto de entrenamiento y otro de test\n",
    "#############################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,labels, test_size=0.3)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# Para hacer la validación, vamos a dividir el conjunto de entrenamiento en dos\n",
    "#############################################################################\n",
    "\n",
    "Xcvtr, Xcvtst, Ycvtr, Ycvtst = train_test_split(x_train,y_train, test_size=0.25)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# Complete el rango de parámetros que vamos a validar\n",
    "#############################################################################\n",
    "depths=np.array([3,7,11, 15])\n",
    "estimators=np.array([10,100,200])\n",
    "\n",
    "BestAccuracy=-1.0\n",
    "BestC = -1\n",
    "BestG = -1\n",
    "    \n",
    "BestAccuracy=-1.0\n",
    "BestEstimator=-1.0\n",
    "\n",
    "    \n",
    "for depth in depths:\n",
    "    for estimator in estimators:    \n",
    "        \n",
    "        print(\"Training parameters: depth=\",depth, \"estimators=\",estimator)\n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        # Entrenamos el algoritmo utilizando como entrenamiento el subconjunto Xcvtr\n",
    "        #############################################################################    \n",
    "        xgbclassifier=XGBClassifier(max_depth=depth, n_estimators=estimator, learning_rate=0.05,subsample=0.9,verbose=2)\n",
    "        xgbclassifier.fit(Xcvtr,Ycvtr)\n",
    "\n",
    "        #############################################################################\n",
    "        # Evaluamos sobre el conjunto Xcvtst\n",
    "        #############################################################################\n",
    "        predictions=xgbclassifier.predict(Xcvtst)\n",
    "        Accuracy = accuracy_score(predictions,Ycvtst)\n",
    "\n",
    "        if(Accuracy> BestAccuracy):\n",
    "            BestAccuracy = Accuracy\n",
    "            BestDepth = depth\n",
    "            BestEstimator = estimator\n",
    "                \n",
    "        print(\"Right Now Best Accuracy CV:\",BestAccuracy, \"Depth:\", BestDepth)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# Finalmente, una vez conocidos los hiperparámetros,\n",
    "# reentrenamos utilizando todo el conjuto de entrenamiento\n",
    "#############################################################################        \n",
    "\n",
    "xgbclassifier = XGBClassifier(max_depth=BestDepth, n_estimators=BestEstimator, learning_rate=0.05,subsample=0.9,verbose=2)\n",
    "xgbclassifier.fit(x_train,y_train)\n",
    "\n",
    "#############################################################################\n",
    "# Y evaluamos el modelo en el conjunto de test\n",
    "#############################################################################        \n",
    "\n",
    "predictions=xgbclassifier.predict(x_test)\n",
    "Accuracy = accuracy_score(predictions,y_test)\n",
    "\n",
    "print(\"Final Accuracy:\",Accuracy)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
